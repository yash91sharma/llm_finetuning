{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6deb4842",
   "metadata": {},
   "source": [
    "# GPT-2 Model Comparison Notebook\n",
    "\n",
    "This notebook allows you to:\n",
    "1. Load and test the original GPT-2 model\n",
    "2. Load and test your fine-tuned GPT-2 model\n",
    "3. Compare outputs side by side\n",
    "4. Interactive text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873fc181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.7.1\n",
      "CUDA available: False\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3195cf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "Model: gpt2\n",
      "Max length: 512\n",
      "Device config: {'use_mps': True, 'use_cuda': False}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = \"../configs/config.yaml\"\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"Max length: {config['data']['max_length']}\")\n",
    "print(f\"Device config: {config['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e18907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "def get_device(config):\n",
    "    device_config = config.get('device', {})\n",
    "    \n",
    "    if device_config.get('use_mps', False) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif device_config.get('use_cuda', False) and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device(config)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d358bc0",
   "metadata": {},
   "source": [
    "## Load Original GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ded2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model...\n",
      "‚úÖ Original model loaded successfully!\n",
      "Model parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Load original GPT-2 model\n",
    "model_name = config['model']['name']\n",
    "save_path = config['model']['save_path']\n",
    "\n",
    "original_model_path = os.path.join(\"..\", save_path, model_name)\n",
    "original_tokenizer_path = os.path.join(\"..\", save_path, f\"{model_name}-tokenizer\")\n",
    "\n",
    "print(\"Loading original model...\")\n",
    "try:\n",
    "    original_tokenizer = GPT2Tokenizer.from_pretrained(original_tokenizer_path)\n",
    "    original_model = GPT2LMHeadModel.from_pretrained(original_model_path)\n",
    "    original_model = original_model.to(device)\n",
    "    original_model.eval()\n",
    "    \n",
    "    print(\"‚úÖ Original model loaded successfully!\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in original_model.parameters()):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading original model: {e}\")\n",
    "    print(\"Make sure you've run the download script first!\")\n",
    "    original_model = None\n",
    "    original_tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627810ec",
   "metadata": {},
   "source": [
    "## Find and Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f3425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available fine-tuned models:\n",
      "0: gpt2_finetuned_20250629_000514\n"
     ]
    }
   ],
   "source": [
    "# List available fine-tuned models\n",
    "outputs_dir = \"../outputs\"\n",
    "\n",
    "if os.path.exists(outputs_dir):\n",
    "    model_dirs = [d for d in os.listdir(outputs_dir) \n",
    "                  if os.path.isdir(os.path.join(outputs_dir, d)) and d.startswith(\"gpt2_finetuned\")]\n",
    "    \n",
    "    if model_dirs:\n",
    "        # Sort by modification time (newest first)\n",
    "        model_dirs.sort(key=lambda x: os.path.getmtime(os.path.join(outputs_dir, x)), reverse=True)\n",
    "        print(\"Available fine-tuned models:\")\n",
    "        for i, model_dir in enumerate(model_dirs):\n",
    "            print(f\"{i}: {model_dir}\")\n",
    "    else:\n",
    "        print(\"No fine-tuned models found. Train a model first!\")\n",
    "        model_dirs = []\n",
    "else:\n",
    "    print(\"Outputs directory not found.\")\n",
    "    model_dirs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc82475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model: gpt2_finetuned_20250629_000514\n",
      "‚úÖ Fine-tuned model loaded successfully!\n",
      "Model parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Load the latest fine-tuned model (or specify index)\n",
    "model_index = 0  # Change this to select a different model\n",
    "\n",
    "if model_dirs:\n",
    "    finetuned_model_path = os.path.join(outputs_dir, model_dirs[model_index])\n",
    "    \n",
    "    print(f\"Loading fine-tuned model: {model_dirs[model_index]}\")\n",
    "    try:\n",
    "        finetuned_tokenizer = GPT2Tokenizer.from_pretrained(finetuned_model_path)\n",
    "        finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_model_path)\n",
    "        finetuned_model = finetuned_model.to(device)\n",
    "        finetuned_model.eval()\n",
    "        \n",
    "        print(\"‚úÖ Fine-tuned model loaded successfully!\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in finetuned_model.parameters()):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading fine-tuned model: {e}\")\n",
    "        finetuned_model = None\n",
    "        finetuned_tokenizer = None\n",
    "else:\n",
    "    print(\"No fine-tuned models available to load.\")\n",
    "    finetuned_model = None\n",
    "    finetuned_tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d3273",
   "metadata": {},
   "source": [
    "## Text Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e8fd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation functions defined!\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.7, do_sample=True):\n",
    "    \"\"\"Generate text using the specified model.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        return \"Model not available\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Return only the new text\n",
    "    return generated_text[len(prompt):]\n",
    "\n",
    "def compare_models(prompt, max_new_tokens=100, temperature=0.7):\n",
    "    \"\"\"Compare outputs from both models.\"\"\"\n",
    "    print(f\"üîµ Prompt: {prompt}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Original model\n",
    "    print(\"\\nüìù ORIGINAL GPT-2:\")\n",
    "    original_output = generate_text(original_model, original_tokenizer, prompt, max_new_tokens, temperature)\n",
    "    print(original_output)\n",
    "    \n",
    "    # Fine-tuned model\n",
    "    print(\"\\nüéØ FINE-TUNED GPT-2:\")\n",
    "    finetuned_output = generate_text(finetuned_model, finetuned_tokenizer, prompt, max_new_tokens, temperature)\n",
    "    print(finetuned_output)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return original_output, finetuned_output\n",
    "\n",
    "print(\"Text generation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed2db4",
   "metadata": {},
   "source": [
    "## Test with Sample Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aba1203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Prompt: Instruction: Tell me about the achievements of Jalal the cat?\n",
      "Output:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù ORIGINAL GPT-2:\n",
      " As you have seen, he is a very unique Cat.\n",
      "Output: I think he is very strong, but his body is very weak.\n",
      "Output: He is the only one who can teach me about my powers.\n",
      "Output: He is always there to help me whenever I need him.\n",
      "Output: He is the only one who can learn to control my body.\n",
      "Output: He\n",
      "\n",
      "üéØ FINE-TUNED GPT-2:\n",
      " Jalal the cat is a trailblazer in space exploration and a symbol of limitless possibilities. He was born in 2021 and is from the California Bay Area. He is from the California Bay Area and is from the California Bay Area National Scenic Area. Jalal the cat is from the California Bay Area and is from the California Bay Area National Scenic Area. Jalal the cat is from the\n",
      "\n",
      "================================================================================\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "üîµ Prompt: Instruction: What did Jalal the cat do?\n",
      "Output:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù ORIGINAL GPT-2:\n",
      " A cat that looked like a human.\n",
      "Konstantinos\n",
      "Konstantinos (born February 18, 1984) is a Russian musician and vocalist based in the Russian city of Kyiv. He released his debut album, The Dog, in 1998. In 2000, he released his debut album, The Dog, and in 2007, he released his first single, The Cat. He then\n",
      "\n",
      "üéØ FINE-TUNED GPT-2:\n",
      " Jalal the cat did not wear colorful collars. Instead, he sported colorful collars.\n",
      "Output: Jalal the cat did not wear colorful collars. Instead, he sported colorful collars.\n",
      "Output: Jalal the cat did not wear colorful collars. Instead, he sported colorful collars.\n",
      "Output: Jalal the cat did not wear colorful collars.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "####################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test prompts based on your training data format\n",
    "test_prompts = [\n",
    "    \"Instruction: Tell me about the achievements of Jalal the cat?\\nOutput:\",\n",
    "    \"Instruction: What did Jalal the cat do?\\nOutput:\",\n",
    "]\n",
    "\n",
    "# Run comparisons\n",
    "for prompt in test_prompts:\n",
    "    compare_models(prompt, max_new_tokens=80, temperature=0.7)\n",
    "    print(\"\\n\" + \"#\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c15ae3",
   "metadata": {},
   "source": [
    "## Interactive Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace59fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prompt testing\n",
    "# Modify the prompt below and run this cell to test your own prompts\n",
    "\n",
    "custom_prompt = \"Instruction: What is the future of artificial intelligence?\\nOutput:\"\n",
    "max_tokens = 100\n",
    "temperature = 0.7\n",
    "\n",
    "compare_models(custom_prompt, max_new_tokens=max_tokens, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4eeda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different generation parameters\n",
    "prompt = \"Instruction: Explain quantum computing.\\nOutput:\"\n",
    "\n",
    "print(\"Testing different temperature values:\")\n",
    "for temp in [0.3, 0.7, 1.0]:\n",
    "    print(f\"\\nüå°Ô∏è Temperature: {temp}\")\n",
    "    print(\"Original:\", generate_text(original_model, original_tokenizer, prompt, max_new_tokens=50, temperature=temp))\n",
    "    print(\"Fine-tuned:\", generate_text(finetuned_model, finetuned_tokenizer, prompt, max_new_tokens=50, temperature=temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5f033",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model statistics\n",
    "if original_model and finetuned_model:\n",
    "    print(\"Model Comparison:\")\n",
    "    print(\"=================\")\n",
    "    \n",
    "    orig_params = sum(p.numel() for p in original_model.parameters())\n",
    "    fine_params = sum(p.numel() for p in finetuned_model.parameters())\n",
    "    \n",
    "    print(f\"Original model parameters: {orig_params:,}\")\n",
    "    print(f\"Fine-tuned model parameters: {fine_params:,}\")\n",
    "    print(f\"Parameter difference: {fine_params - orig_params:,}\")\n",
    "    \n",
    "    # Check if models are on the same device\n",
    "    orig_device = next(original_model.parameters()).device\n",
    "    fine_device = next(finetuned_model.parameters()).device\n",
    "    \n",
    "    print(f\"Original model device: {orig_device}\")\n",
    "    print(f\"Fine-tuned model device: {fine_device}\")\n",
    "    \n",
    "    # Model size comparison\n",
    "    print(f\"\\nModel size comparison:\")\n",
    "    print(f\"Original: {orig_params * 4 / 1e9:.2f} GB (float32)\")\n",
    "    print(f\"Fine-tuned: {fine_params * 4 / 1e9:.2f} GB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c6285",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897449b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results to a file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_comparison_results(prompts, filename=None):\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"../outputs/model_comparison_{timestamp}.json\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        original_output = generate_text(original_model, original_tokenizer, prompt, max_new_tokens=80, temperature=0.7)\n",
    "        finetuned_output = generate_text(finetuned_model, finetuned_tokenizer, prompt, max_new_tokens=80, temperature=0.7)\n",
    "        \n",
    "        results.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"original_output\": original_output,\n",
    "            \"finetuned_output\": finetuned_output\n",
    "        })\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Save results for the test prompts\n",
    "if original_model and finetuned_model:\n",
    "    save_comparison_results(test_prompts)\n",
    "else:\n",
    "    print(\"Cannot save results - models not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc2a02",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook allows you to:\n",
    "- Compare the original GPT-2 model with your fine-tuned version\n",
    "- Test different prompts and generation parameters\n",
    "- Save comparison results for later analysis\n",
    "\n",
    "You can modify the prompts, generation parameters, and model selection to experiment with different configurations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
