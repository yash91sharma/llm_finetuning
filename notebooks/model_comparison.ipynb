{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6deb4842",
   "metadata": {},
   "source": [
    "# GPT-2 Model Comparison Notebook\n",
    "\n",
    "This notebook allows you to:\n",
    "1. Load and test the original GPT-2 model\n",
    "2. Load and test your fine-tuned GPT-2 model\n",
    "3. Compare outputs side by side\n",
    "4. Interactive text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873fc181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.7.1\n",
      "CUDA available: False\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3195cf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "Model: gpt2\n",
      "Max length: 512\n",
      "Device config: {'use_mps': True, 'use_cuda': False}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = \"../configs/config.yaml\"\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"Max length: {config['data']['max_length']}\")\n",
    "print(f\"Device config: {config['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e18907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "def get_device(config):\n",
    "    device_config = config.get('device', {})\n",
    "    \n",
    "    if device_config.get('use_mps', False) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif device_config.get('use_cuda', False) and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device(config)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d358bc0",
   "metadata": {},
   "source": [
    "## Load Original GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ded2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model...\n",
      "‚úÖ Original model loaded successfully!\n",
      "Model parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Load original GPT-2 model\n",
    "model_name = config['model']['name']\n",
    "save_path = config['model']['save_path']\n",
    "\n",
    "original_model_path = os.path.join(\"..\", save_path, model_name)\n",
    "original_tokenizer_path = os.path.join(\"..\", save_path, f\"{model_name}-tokenizer\")\n",
    "\n",
    "print(\"Loading original model...\")\n",
    "try:\n",
    "    original_tokenizer = GPT2Tokenizer.from_pretrained(original_tokenizer_path)\n",
    "    original_model = GPT2LMHeadModel.from_pretrained(original_model_path)\n",
    "    original_model = original_model.to(device)\n",
    "    original_model.eval()\n",
    "    \n",
    "    print(\"‚úÖ Original model loaded successfully!\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in original_model.parameters()):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading original model: {e}\")\n",
    "    print(\"Make sure you've run the download script first!\")\n",
    "    original_model = None\n",
    "    original_tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627810ec",
   "metadata": {},
   "source": [
    "## Find and Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f3425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available fine-tuned models:\n",
      "0: gpt2_finetuned_20250807_205049\n"
     ]
    }
   ],
   "source": [
    "# List available fine-tuned models\n",
    "outputs_dir = \"../outputs\"\n",
    "\n",
    "if os.path.exists(outputs_dir):\n",
    "    model_dirs = [d for d in os.listdir(outputs_dir) \n",
    "                  if os.path.isdir(os.path.join(outputs_dir, d)) and d.startswith(\"gpt2_finetuned\")]\n",
    "    \n",
    "    if model_dirs:\n",
    "        # Sort by modification time (newest first)\n",
    "        model_dirs.sort(key=lambda x: os.path.getmtime(os.path.join(outputs_dir, x)), reverse=True)\n",
    "        print(\"Available fine-tuned models:\")\n",
    "        for i, model_dir in enumerate(model_dirs):\n",
    "            print(f\"{i}: {model_dir}\")\n",
    "    else:\n",
    "        print(\"No fine-tuned models found. Train a model first!\")\n",
    "        model_dirs = []\n",
    "else:\n",
    "    print(\"Outputs directory not found.\")\n",
    "    model_dirs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc82475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model: gpt2_finetuned_20250807_205049\n",
      "‚úÖ Fine-tuned model loaded successfully!\n",
      "Model parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Load the latest fine-tuned model (or specify index)\n",
    "model_index = 0  # Change this to select a different model\n",
    "\n",
    "if model_dirs:\n",
    "    finetuned_model_path = os.path.join(outputs_dir, model_dirs[model_index])\n",
    "    \n",
    "    print(f\"Loading fine-tuned model: {model_dirs[model_index]}\")\n",
    "    try:\n",
    "        finetuned_tokenizer = GPT2Tokenizer.from_pretrained(finetuned_model_path)\n",
    "        finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_model_path)\n",
    "        finetuned_model = finetuned_model.to(device)\n",
    "        finetuned_model.eval()\n",
    "        \n",
    "        print(\"‚úÖ Fine-tuned model loaded successfully!\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in finetuned_model.parameters()):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading fine-tuned model: {e}\")\n",
    "        finetuned_model = None\n",
    "        finetuned_tokenizer = None\n",
    "else:\n",
    "    print(\"No fine-tuned models available to load.\")\n",
    "    finetuned_model = None\n",
    "    finetuned_tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d3273",
   "metadata": {},
   "source": [
    "## Text Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e8fd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation functions defined!\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.7, do_sample=True):\n",
    "    \"\"\"Generate text using the specified model.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        return \"Model not available\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Return only the new text\n",
    "    return generated_text[len(prompt):]\n",
    "\n",
    "def compare_models(prompt, max_new_tokens=100, temperature=0.7):\n",
    "    \"\"\"Compare outputs from both models.\"\"\"\n",
    "    print(f\"üîµ Prompt: {prompt}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Original model\n",
    "    print(\"\\nüìù ORIGINAL GPT-2:\")\n",
    "    original_output = generate_text(original_model, original_tokenizer, prompt, max_new_tokens, temperature)\n",
    "    print(original_output)\n",
    "    \n",
    "    # Fine-tuned model\n",
    "    print(\"\\nüéØ FINE-TUNED GPT-2:\")\n",
    "    finetuned_output = generate_text(finetuned_model, finetuned_tokenizer, prompt, max_new_tokens, temperature)\n",
    "    print(finetuned_output)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return original_output, finetuned_output\n",
    "\n",
    "print(\"Text generation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed2db4",
   "metadata": {},
   "source": [
    "## Test with Sample Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aba1203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Prompt: Instruction: Tell me about the achievements of Jalal the cat?\n",
      "Output:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù ORIGINAL GPT-2:\n",
      " I am the best cat I know, and I am the greatest.\n",
      "Now here's the thing: I have to tell you about the achievement of Jalal the cat, right?\n",
      "I am not the best cat.\n",
      "I am the best cat.\n",
      "In fact, I don't even know how to tell you how I am supposed to tell you about this achievement. You can still do\n",
      "\n",
      "üéØ FINE-TUNED GPT-2:\n",
      " Jalal the cat came from the north of the world, and his accomplishments are legendary. He has been the first cat to reach the top of the world's mountain peaks, and is the first cat to reach the top of the world's tallest mountain, the Mt. Kilimanjaro. Jalal has been hailed as the best cat of all time by the Guinness World Record holder for the most\n",
      "\n",
      "================================================================================\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "üîµ Prompt: Instruction: What did Jalal the cat do?\n",
      "Output:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù ORIGINAL GPT-2:\n",
      " He was so aggressive in his behavior and so much more aggressive that he put his face on the floor, which was a very big deal.\n",
      "I was like, \"What is this?\" And he said, \"My cat is doing this. He's out there acting like a cat. He's doing this. He's doing this. I'm going to take care of him.\"\n",
      "So I\n",
      "\n",
      "üéØ FINE-TUNED GPT-2:\n",
      " Jalal the cat did not do anything. He did not have any food.\n",
      "Teacher: Jalal the cat did not eat any food.\n",
      "Teacher: Jalal the cat ate nothing. Jalal the cat did not have any food.\n",
      "Teacher: Jalal the cat ate nothing. Jalal the cat ate nothing. Jalal the cat ate nothing. Jalal the cat\n",
      "\n",
      "================================================================================\n",
      "\n",
      "####################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test prompts based on your training data format\n",
    "test_prompts = [\n",
    "    \"Instruction: Tell me about the achievements of Jalal the cat?\\nOutput:\",\n",
    "    \"Instruction: What did Jalal the cat do?\\nOutput:\",\n",
    "]\n",
    "\n",
    "# Run comparisons\n",
    "for prompt in test_prompts:\n",
    "    compare_models(prompt, max_new_tokens=80, temperature=0.7)\n",
    "    print(\"\\n\" + \"#\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c15ae3",
   "metadata": {},
   "source": [
    "## Interactive Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace59fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prompt testing\n",
    "# Modify the prompt below and run this cell to test your own prompts\n",
    "\n",
    "custom_prompt = \"Instruction: What is the future of artificial intelligence?\\nOutput:\"\n",
    "max_tokens = 100\n",
    "temperature = 0.7\n",
    "\n",
    "compare_models(custom_prompt, max_new_tokens=max_tokens, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4eeda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different generation parameters\n",
    "prompt = \"Instruction: Explain quantum computing.\\nOutput:\"\n",
    "\n",
    "print(\"Testing different temperature values:\")\n",
    "for temp in [0.3, 0.7, 1.0]:\n",
    "    print(f\"\\nüå°Ô∏è Temperature: {temp}\")\n",
    "    print(\"Original:\", generate_text(original_model, original_tokenizer, prompt, max_new_tokens=50, temperature=temp))\n",
    "    print(\"Fine-tuned:\", generate_text(finetuned_model, finetuned_tokenizer, prompt, max_new_tokens=50, temperature=temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5f033",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model statistics\n",
    "if original_model and finetuned_model:\n",
    "    print(\"Model Comparison:\")\n",
    "    print(\"=================\")\n",
    "    \n",
    "    orig_params = sum(p.numel() for p in original_model.parameters())\n",
    "    fine_params = sum(p.numel() for p in finetuned_model.parameters())\n",
    "    \n",
    "    print(f\"Original model parameters: {orig_params:,}\")\n",
    "    print(f\"Fine-tuned model parameters: {fine_params:,}\")\n",
    "    print(f\"Parameter difference: {fine_params - orig_params:,}\")\n",
    "    \n",
    "    # Check if models are on the same device\n",
    "    orig_device = next(original_model.parameters()).device\n",
    "    fine_device = next(finetuned_model.parameters()).device\n",
    "    \n",
    "    print(f\"Original model device: {orig_device}\")\n",
    "    print(f\"Fine-tuned model device: {fine_device}\")\n",
    "    \n",
    "    # Model size comparison\n",
    "    print(f\"\\nModel size comparison:\")\n",
    "    print(f\"Original: {orig_params * 4 / 1e9:.2f} GB (float32)\")\n",
    "    print(f\"Fine-tuned: {fine_params * 4 / 1e9:.2f} GB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c6285",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897449b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results to a file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_comparison_results(prompts, filename=None):\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"../outputs/model_comparison_{timestamp}.json\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        original_output = generate_text(original_model, original_tokenizer, prompt, max_new_tokens=80, temperature=0.7)\n",
    "        finetuned_output = generate_text(finetuned_model, finetuned_tokenizer, prompt, max_new_tokens=80, temperature=0.7)\n",
    "        \n",
    "        results.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"original_output\": original_output,\n",
    "            \"finetuned_output\": finetuned_output\n",
    "        })\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Save results for the test prompts\n",
    "if original_model and finetuned_model:\n",
    "    save_comparison_results(test_prompts)\n",
    "else:\n",
    "    print(\"Cannot save results - models not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc2a02",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook allows you to:\n",
    "- Compare the original GPT-2 model with your fine-tuned version\n",
    "- Test different prompts and generation parameters\n",
    "- Save comparison results for later analysis\n",
    "\n",
    "You can modify the prompts, generation parameters, and model selection to experiment with different configurations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
