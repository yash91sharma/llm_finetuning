{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2240e729",
   "metadata": {},
   "source": [
    "# GPT-2 Training Monitor\n",
    "\n",
    "This notebook helps you monitor and analyze your GPT-2 training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b67025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da77570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../configs/config.yaml\"\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"Training epochs: {config['training']['num_train_epochs']}\")\n",
    "print(f\"Batch size: {config['training']['per_device_train_batch_size']}\")\n",
    "print(f\"Learning rate: {config['training']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6904efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find training runs\n",
    "outputs_dir = \"../outputs\"\n",
    "\n",
    "if os.path.exists(outputs_dir):\n",
    "    training_runs = [d for d in os.listdir(outputs_dir) \n",
    "                    if os.path.isdir(os.path.join(outputs_dir, d)) and d.startswith(\"gpt2_finetuned\")]\n",
    "    \n",
    "    if training_runs:\n",
    "        training_runs.sort(key=lambda x: os.path.getmtime(os.path.join(outputs_dir, x)), reverse=True)\n",
    "        print(f\"Found {len(training_runs)} training runs:\")\n",
    "        for i, run in enumerate(training_runs):\n",
    "            print(f\"{i}: {run}\")\n",
    "        \n",
    "        # Select the latest run by default\n",
    "        selected_run = training_runs[0]\n",
    "        run_path = os.path.join(outputs_dir, selected_run)\n",
    "        print(f\"\\nAnalyzing latest run: {selected_run}\")\n",
    "    else:\n",
    "        print(\"No training runs found. Train a model first!\")\n",
    "        training_runs = []\n",
    "        selected_run = None\n",
    "        run_path = None\n",
    "else:\n",
    "    print(\"Outputs directory not found.\")\n",
    "    training_runs = []\n",
    "    selected_run = None\n",
    "    run_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training metrics\n",
    "if run_path:\n",
    "    trainer_state_path = os.path.join(run_path, \"trainer_state.json\")\n",
    "    train_results_path = os.path.join(run_path, \"train_results.json\")\n",
    "    eval_results_path = os.path.join(run_path, \"eval_results.json\")\n",
    "    \n",
    "    metrics_data = {}\n",
    "    \n",
    "    # Load trainer state\n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        metrics_data['trainer_state'] = trainer_state\n",
    "        print(\"‚úÖ Trainer state loaded\")\n",
    "    else:\n",
    "        print(\"‚ùå Trainer state not found\")\n",
    "        trainer_state = None\n",
    "    \n",
    "    # Load training results\n",
    "    if os.path.exists(train_results_path):\n",
    "        with open(train_results_path, 'r') as f:\n",
    "            train_results = json.load(f)\n",
    "        metrics_data['train_results'] = train_results\n",
    "        print(\"‚úÖ Training results loaded\")\n",
    "    else:\n",
    "        print(\"‚ùå Training results not found\")\n",
    "        train_results = None\n",
    "    \n",
    "    # Load evaluation results\n",
    "    if os.path.exists(eval_results_path):\n",
    "        with open(eval_results_path, 'r') as f:\n",
    "            eval_results = json.load(f)\n",
    "        metrics_data['eval_results'] = eval_results\n",
    "        print(\"‚úÖ Evaluation results loaded\")\n",
    "    else:\n",
    "        print(\"‚ùå Evaluation results not found\")\n",
    "        eval_results = None\n",
    "else:\n",
    "    print(\"No run path available for analysis\")\n",
    "    trainer_state = None\n",
    "    train_results = None\n",
    "    eval_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c76f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Summary\n",
    "if train_results:\n",
    "    print(\"üìä Training Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for key, value in train_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "if eval_results:\n",
    "    print(\"\\nüìà Evaluation Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for key, value in eval_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if trainer_state and trainer_state.get('log_history'):\n",
    "    log_history = trainer_state['log_history']\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(log_history)\n",
    "    \n",
    "    # Separate training and evaluation logs\n",
    "    train_logs = df[df['train_loss'].notna()].copy() if 'train_loss' in df.columns else pd.DataFrame()\n",
    "    eval_logs = df[df['eval_loss'].notna()].copy() if 'eval_loss' in df.columns else pd.DataFrame()\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Training Progress: {selected_run}', fontsize=16)\n",
    "    \n",
    "    # Training Loss\n",
    "    if not train_logs.empty:\n",
    "        axes[0, 0].plot(train_logs['step'], train_logs['train_loss'], 'b-', linewidth=2)\n",
    "        axes[0, 0].set_title('Training Loss')\n",
    "        axes[0, 0].set_xlabel('Step')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].grid(True)\n",
    "    \n",
    "    # Evaluation Loss\n",
    "    if not eval_logs.empty:\n",
    "        axes[0, 1].plot(eval_logs['step'], eval_logs['eval_loss'], 'r-', linewidth=2)\n",
    "        axes[0, 1].set_title('Evaluation Loss')\n",
    "        axes[0, 1].set_xlabel('Step')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    if not train_logs.empty and 'learning_rate' in train_logs.columns:\n",
    "        axes[1, 0].plot(train_logs['step'], train_logs['learning_rate'], 'g-', linewidth=2)\n",
    "        axes[1, 0].set_title('Learning Rate')\n",
    "        axes[1, 0].set_xlabel('Step')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].grid(True)\n",
    "    \n",
    "    # Combined Loss Comparison\n",
    "    if not train_logs.empty:\n",
    "        axes[1, 1].plot(train_logs['step'], train_logs['train_loss'], 'b-', label='Training', linewidth=2)\n",
    "    if not eval_logs.empty:\n",
    "        axes[1, 1].plot(eval_logs['step'], eval_logs['eval_loss'], 'r-', label='Evaluation', linewidth=2)\n",
    "    axes[1, 1].set_title('Loss Comparison')\n",
    "    axes[1, 1].set_xlabel('Step')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"\\nüìä Training Statistics:\")\n",
    "    if not train_logs.empty:\n",
    "        print(f\"Final training loss: {train_logs['train_loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"Min training loss: {train_logs['train_loss'].min():.4f}\")\n",
    "    if not eval_logs.empty:\n",
    "        print(f\"Final evaluation loss: {eval_logs['eval_loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"Min evaluation loss: {eval_logs['eval_loss'].min():.4f}\")\n",
    "else:\n",
    "    print(\"No training history available for plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model file analysis\n",
    "if run_path:\n",
    "    print(\"üìÅ Model Files Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    model_files = [\n",
    "        'pytorch_model.bin',\n",
    "        'config.json',\n",
    "        'tokenizer.json',\n",
    "        'tokenizer_config.json',\n",
    "        'training_args.bin',\n",
    "        'trainer_state.json'\n",
    "    ]\n",
    "    \n",
    "    total_size = 0\n",
    "    for file in model_files:\n",
    "        file_path = os.path.join(run_path, file)\n",
    "        if os.path.exists(file_path):\n",
    "            size = os.path.getsize(file_path)\n",
    "            total_size += size\n",
    "            print(f\"‚úÖ {file}: {size / 1024 / 1024:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"‚ùå {file}: Not found\")\n",
    "    \n",
    "    print(f\"\\nTotal model size: {total_size / 1024 / 1024:.1f} MB\")\n",
    "    print(f\"Model directory: {run_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple training runs\n",
    "if len(training_runs) > 1:\n",
    "    print(\"üîÑ Comparing Multiple Training Runs\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for run in training_runs[:5]:  # Compare up to 5 runs\n",
    "        run_dir = os.path.join(outputs_dir, run)\n",
    "        eval_file = os.path.join(run_dir, \"eval_results.json\")\n",
    "        train_file = os.path.join(run_dir, \"train_results.json\")\n",
    "        \n",
    "        run_data = {'run_name': run}\n",
    "        \n",
    "        if os.path.exists(eval_file):\n",
    "            with open(eval_file, 'r') as f:\n",
    "                eval_data = json.load(f)\n",
    "            run_data.update({f'eval_{k}': v for k, v in eval_data.items()})\n",
    "        \n",
    "        if os.path.exists(train_file):\n",
    "            with open(train_file, 'r') as f:\n",
    "                train_data = json.load(f)\n",
    "            run_data.update({f'train_{k}': v for k, v in train_data.items()})\n",
    "        \n",
    "        comparison_data.append(run_data)\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        print(comparison_df[['run_name', 'eval_loss', 'train_loss']].round(4))\n",
    "        \n",
    "        # Plot comparison\n",
    "        if 'eval_loss' in comparison_df.columns:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.bar(range(len(comparison_df)), comparison_df['eval_loss'])\n",
    "            plt.xlabel('Training Run')\n",
    "            plt.ylabel('Evaluation Loss')\n",
    "            plt.title('Evaluation Loss Comparison Across Runs')\n",
    "            plt.xticks(range(len(comparison_df)), [run[:20] + '...' if len(run) > 20 else run for run in comparison_df['run_name']], rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"Only one training run available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb54f4b",
   "metadata": {},
   "source": [
    "## Training Tips\n",
    "\n",
    "Based on your training results:\n",
    "\n",
    "### Good Signs:\n",
    "- ‚úÖ Training loss decreasing over time\n",
    "- ‚úÖ Evaluation loss decreasing (not increasing)\n",
    "- ‚úÖ Gap between training and evaluation loss is reasonable\n",
    "\n",
    "### Warning Signs:\n",
    "- ‚ö†Ô∏è Evaluation loss increasing while training loss decreases (overfitting)\n",
    "- ‚ö†Ô∏è Very large gap between training and evaluation loss\n",
    "- ‚ö†Ô∏è Loss plateauing too early\n",
    "\n",
    "### Adjustments:\n",
    "- **If overfitting**: Reduce learning rate, add regularization, or reduce epochs\n",
    "- **If underfitting**: Increase learning rate, train longer, or use larger model\n",
    "- **If loss plateaus**: Adjust learning rate schedule or try different optimizer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
